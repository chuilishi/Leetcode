# 爬虫

### Session与Cookie

Session**(会话)**是Cookie在服务器上作用的基础

### urllib.request

request.urlopen( )方法

打开一个链接

返回一个HTTPResponse对象，它包含了read( )、readinto( )、getheader(name)、getheaders( )、fileno( )等方法， 以及msg、version、status、reason、debuglevel、closed等属性

可以添加一些参数:

header: 设置header

method: 请求方法

data: 携带的数据

timeout: 设置访问超时时间



### data

data的内容由请求头中的Content-Type决定.

其他的自己去搜,

文件类的需要先将数据转为二进制流

```
data = bytes(parse.urlencode(dict), encoding='utf-8')
```

然后再发送request:

```
req = request.Request(url=url, data=data, headers=headers, method='POST')
```

### Handler处理高级Http内容(Cookies,代理等)

首先，介绍一下 `urllib.request` 模块里的 `BaseHandler` 类，它是所有其他 Handler 的父类，它提供了最基本的方法，例如 `default_open`、`protocol_request` 等。

接下来，就有各种 Handler 子类继承这个 `BaseHandler` 类，举例如下。

- `HTTPDefaultErrorHandler` 用于处理 HTTP 响应错误，错误都会抛出 `HTTPError` 类型的异常。
- `HTTPRedirectHandler` 用于处理重定向。
- `HTTPCookieProcessor` 用于处理 Cookies。
- `ProxyHandler` 用于设置代理，默认代理为空。
- `HTTPPasswordMgr` 用于管理密码，它维护了用户名和密码的表。
- `HTTPBasicAuthHandler` 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。

### 账户密码处理

例如`HTTPBasicAuthHandle`的使用:

```
# 密码Handler的参数对象
p = HTTPPasswordMgrWithDefaultRealm() 
# 添加密码
p.add_password(None, url, username, password)
# 创建Handler
auth_handler = HTTPBasicAuthHandler(p)
# 创建opener
opener = build_opener(auth_handler)
try:
	# 相当于一个带着密码的urlopen()
	result = opener.open(url)
	html = result.read().decode('utf-8')
	print(html)
except URLError as e:
	print(e.reason)
```

### 使用代理

`ProxyHandler`

```
# 创建代理池
proxy_handler = ProxyHandler({
	'http':'http://xxxx'
	'https':'https://xxxx'
})
opener = build_opener(proxy_handler)
try:
	response =		
	opener.open('https://www.baidu.com')
	print(response.read().decode('utf-8'))
except URLError as e:
	print(e.reason)
```

### Cookie相关

获取**cookie**

```
cookie = http.cookiejar.CookieJar()
handler = request.HTTPCookieProcessor(cookie)
opener = request.build_opener(handler)
response = opener.open('https://www.baidu.com')
for item in cookie:
	print(item.name + "=" + item.value)
    
# 输出结果类似:
BAIDUID=A09E6C4E38753531B9FB4C60CE9FDFCB:FG=1
BIDUPSID=A09E6C4E387535312F8AA46280C6C502
H_PS_PSSID=31358_1452_31325_21088_31110_31253_31605_31271_31463_30823
PSTM=1590854698
BDSVRTM=10
```

**保存在本地**

```
filename = 'cookie.txt'
# MozillaCookieJar用来保存Mozilla型浏览器的Cookie格式
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
```

### error

URLError:

- `code`：返回 HTTP 状态码，比如 404 表示网页不存在，500 表示服务器内部错误等。
- `reason`：同父类一样，用于返回错误的原因。
- `headers`：返回请求头。

HTTPError:

URLError的子类



### parse解析链接

 处理url的模块:

它支持如下协议的 URL 处理：`file`、`ftp`、`gopher`、`hdl`、`http`、`https`、`imap`、`mailto`、`mms`、`news`、`nntp`、`prospero`、`rsync`、`rtsp`、`rtspu`、`sftp`、`sip`、`sips`、`snews`、`svn`、`svn+ssh`、`telnet` 和 `wais`。本节中，我们介绍一下该模块中常用的方法来看一下它的便捷之处。

**parse** url识别和分段

parse.urlparse('某url')

将url分成

 `scheme`、`netloc`、`path`、`params`、`query` 和 `fragment`

6个部分

**urlunparse("")**

一个相反的方法,将各个部分组成一个url(参数必须要有6个)

**urlsplit()**

与urlparse差不多,但是会将`params`参数部分与url主体合并

**urlunsplit()**

长度为5

**urljoin('')**

前后两个字符串,后面有更新的部分就接上

urljoin(''https://www.baidu.com/index.html;user?id=5#commet'',"?id=100")

结果就是id被替换成100,后面的消失



**urllencode(params)**  url的序列化

将一个参数的字典序列化成一个符合url中GET请求参数的形式

params = {

​	'name': 'germey',

​	'age': 25

}

结果是: fruit=apple&number=3

**parse_qs()**  url的反序列化

```
query = 'name=germey&age=25'
print(parse_qs(query))
```

运行结果如下：

```
{'name': ['germey'], 'age': ['25']}
```

**parse_qsl()**  结果是元组形式

`[('name', 'germey'), ('age', '25')]`

**quote()**

含有中文字符的url放进去,转化为url编码( 百分号后面跟着两个16进制数字代表在Unicode中的码 )

**unquote()**

同上



### Robots协议

网站对能爬什么内容的规定

```
User-agent: *
Disallow: /
Allow: /public/
```

下面我们看一个 robots.txt 的样例：

```
User-agent: *
Disallow: /
Allow: /public/
```

这实现了对所有搜索爬虫只允许爬取 public 目录的功能，将上述内容保存成 robots.txt 文件，放在网站的根目录下，和网站的入口文件（比如 index.php、index.html 和 index.jsp 等）放在一起。

允许一个爬虫访问所有,阻止其他爬虫:

```
User-agent: WebCrawler
Disallow:
User-agent: *
Disallow: /
```

### robotparser

了解 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt 了。该模块提供了一个类 `RobotFileParser`，它可以根据某网站的 robots.txt 文件来判断一个爬虫是否有权限来爬取这个网页。

该类用起来非常简单，只需要在构造方法里传入 robots.txt 的链接即可。首先看一下它的声明：

```
urllib.robotparser.RobotFileParser(url='')
```

下面列出了这个类常用的几个方法。

- `set_url`：用来设置 robots.txt 文件的链接。如果在创建 `RobotFileParser` 对象时传入了链接，那么就不需要再使用这个方法设置了。
- `read`：读取 robots.txt 文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 `False`，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。
- `parse`：用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。
- `can_fetch`：该方法用两个参数，第一个是 `User-Agent`，第二个是要抓取的 URL。返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 `True` 或 `False`。
- `mtime`：返回的是上次抓取和分析 robots.txt 的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。
- `modified`：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。

### 爬虫名称

大家可能会疑惑，爬虫名是从哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作 BaiduSpider。表 2- 列出了一些常见搜索爬虫的名称及对应的网站。

表 一些常见搜索爬虫的名称及其对应的网站

| 爬虫名称    | 名称      | 网站              |
| :---------- | :-------- | :---------------- |
| BaiduSpider | 百度      | www.baidu.com     |
| Googlebot   | 谷歌      | www.google.com    |
| 360Spider   | 360 搜索  | www.so.com        |
| YodaoBot    | 有道      | www.youdao.com    |
| ia_archiver | Alexa     | www.alexa.cn      |
| Scooter     | altavista | www.altavista.com |
| Bingbot     | 必应      | www.bing.com      |



## requests库

**一些请求**

urllib 库中的 urlopen 方法实际上是以 GET 方式请求网页，而 requests 中相应的方法就是 get 方法,非常直观

response = **request.get**( 'url' , params = data, headers = header )   params和headers都是字典,get方法自动处理

**response.json( )** 将 返回值为JSON格式的str数据转为字典

(不是则报错)

还有post, 等其他方法都可以直接用

**二进制数据**:**response.content** 

用content可以读取二进制数据 ( text 是自动转为文字消息 )

```
image = requests.get("http://xxx.jpg")
with open('image.jpg','wb') as img:
    img.write(image.content)
```

**状态码查询**

内置了一个requests.code对象用来方便查询

```
r = requsets.get( 'https://ssr1.scrape.center/')
if r.status_code == requests.code.ok:
	print("success")
else :
	exit()
```

**cookies**

包含在headers中

jar = **requests.cookies.RequestsCookieJar( )**

这是request的cookie对象

jar.set(key, value)  // 给cookie对象设置值

request.get('url', cookies=jar )   //请求时可以这样带上cookie

**Session**

一个会话

普通的多次请求是纯粹的多次请求,每次请求完全独立,获取的cookie不能相互使用 ( 可以自己手动调整cookie,但是很麻烦 )

使用session的get,相当于浏览器的一个新标签页,信息都被保存了.

```
s = requests.session()
s.get("https://httpbin.org/cookies/set/number/123456789")
cookies = s.get('https://httpbin.org/cookies')
print(cookies.text)
```

**证书**

verify=False 忽略ssl认证

```
requests.get('https://ssr2.scrape.center/', verify=False)
```

这样会给出警告,可以通过两种方式忽略:

```
from requests.packages import urllib3
urllib3.disable_warnings()
import logging
logging.captureWarnings(True)
```

**超时设置**

get请求的 timeout属性

```
r = requests.get('https://httpbin.org/get', timeout=1)
```

也可以`timeout=(20, 400)`

前面的代表连接秒数,后面代表传输秒数

**身份认证**

```
auth=HTTPBasicAuth('admin', 'admin')
```

成功返回200,失败返回401

还有一些其他认证方式,OAuth等

**代理设置proxies**

```
proxies = {
  'http': 'http://10.10.10.10:1080',
  'https': 'http://10.10.10.10:1080',
}
requests.get('https://httpbin.org/get', proxies=proxies)
```

若代理需要使用上文所述的身份认证，可以使用类似 `http://user:password@host:port/ `这样的语法来设置代理



除了基本的 HTTP 代理外，requests 还支持 SOCKS 协议的代理。

首先，需要安装 socks 这个库：

```
pip3 install "requests[socks]"
```

然后就可以使用 SOCKS 协议代理了，示例如下：

```
import requests

proxies = {
    'http': 'socks5://user:password@host:port',
    'https': 'socks5://user:password@host:port'
}
requests.get('https://httpbin.org/get', proxies=proxies)
```



**Prepared Request**

get请求的内部实现: 构造一个Prepared Request对象,调用session的send( Prepared Request ) 

```
s = Session()
req = Request('POST', url, data=data, headers=headers)  //构建Request对象
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```

这样可以随时修改Request,更加灵活



**logging**

一个打印错误的很方便的库,可以分严重等级打印(INFO,WARNING,ERROR等)

也可以像printf一样用占位符打印

`logging.info('hey, my name is %s',name)`

**json**库

将一个字典输出到json文件中:

```
json.dump(data,open(data_path,'w',encoding='utf-8'),ensure_ascii=False,indent=1)
```

ensure_ascii=False  保证了中文字符的正常显示

indent=1  一行缩进

将json格式的字符串转为

```
json.dumps(data)
```

### 编码问题

encode 用来将字符转换为二进制数据

decode 按某种编码将二进制数据转为字符

网上爬下来的数据:

`html = requests.get("http://www.baidu.com")`

html.text  是按照get来的数据的header中的encoding属性来解析文字

html.content 是原始二进制数据,可以自己用decode()解码



像这样的:

```
\u00e6\u0097\u00a7\u00e7\u0089\u0088
```

是将别人的二进制代码当成实际字符来转了,可以试着先decode回二进制代码,再encode到真正的字符

**总结**: open()之后write的源一定要是其对应的编码,把握好二进制数据和各个码的关系就ok



### XPath

`库:lxml`

打开HTML:

```
text = '''
<div>

 </div>
'''
html = etree.HTML(text)
result = etree.tostring(html)
print(result.decode('utf-8'))
```

选取所有节点:

```
from lxml import etree
html = etree.parse('./test.html', etree.HTMLParser())
result = html.xpath('//*')
print(result)
```



### playwright

```
//运行完毕的代码保存到script.py中
playwright codegen -o script.py 
//访问url
https://oj.dgut.edu.cn/status
//将浏览器状态保存到state.json中
--save-storage=state.json
```

```
playwright codegen -o script.py https://oj.dgut.edu.cn/status --load-storage=state.json //改为load就是加载之前的状态
```

Page object有一些方法可以获取页面的内容，例如：

- page.content()：返回页面的完整HTML
- page.innerText(selector)：返回选择器匹配的元素的内部文本
- page.innerHTML(selector)：返回选择器匹配的元素的内部HTML
- page.textContent(selector)：返回选择器匹配的元素的文本内容

你也可以使用page.evaluate()方法在页面上执行任意JavaScript代码，并返回表达式的值

**主要步骤**

```
with sync_playwright() as p:
	#打开一个浏览器
	browser = p.chromium.launch(headless=False)
	#加载context
	context = browser.newcontext(storage_state='state.json')
	#打开一个页面
	page = browser.new_page()
	page.goto('https://oj.dgut.edu.cn/status')
	#下面是一些page的操作
	page.content()#返回完整HTML
	

	#这里的selector包括css,XPath,文本,ID,属性选择器,可以用>>隔开多个引擎,例如
	css=form>>text="Log in"
	page.textContent(selector)
	page.innerText(selector)
#innerText只会返回用户能看到的text
textContent可以返回被隐藏起来的内容(display:None)
	
```

